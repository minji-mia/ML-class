{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T16:17:25.411373Z",
     "start_time": "2018-10-19T16:17:23.660697Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from sklearn import svm\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (10, 8) # set default figure size, 8in by 6in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 04: Support Vector Machines\n",
    "\n",
    "**Due Date:** Friday 10/26/2018 (by midnight)\n",
    "\n",
    "\n",
    "## Introduction \n",
    "In this exercise you will be using support vector machines (SVMs) with various example 2D datasets.  Exerimenting with\n",
    "these datasets will help you gain an intuition of how SVMs work and how to use a Gaussian kernel with SVMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Dataset 1\n",
    "\n",
    "We will begin with a 2D example dataset which can be separated by a linear boundary.  In the following cell, we load the dataset\n",
    "for you.  Plot the dataset using '+' symbols for the positive examples (the 1's) and 'o' symbols for the negative examples\n",
    "(the 0's)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T16:17:25.438624Z",
     "start_time": "2018-10-19T16:17:25.418070Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/assg-04-data1.csv', names=['x1', 'x2', 'y'])\n",
    "X = data[['x1', 'x2']].as_matrix()\n",
    "y = data.y.as_matrix()\n",
    "m,n = X.shape # m = number of training examples, n = number of features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T16:17:26.227762Z",
     "start_time": "2018-10-19T16:17:25.443679Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot the data to visualize it here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that there is a point that appears like it may be a mistake or an outlier in the data.  Notice the positive\n",
    "example on the far left at about (0.1, 4.1).  As a part of this exercise, you will see how this outlier affects the SVM decision\n",
    "boundary.\n",
    "\n",
    "You will try using different values of the $C$ parameter with a linear SVM classifier.  We discussed that the $C$ parameter in the\n",
    "usual formulation of the SVM model corresponds to the $lambda$ parameter we used to specify the level of regularization we wanted\n",
    "to be used in the model that was found.  Recall back from week 5 that regularization is a penalty term we add when parameters of a model get too big.  The idea is that such penalities penalize overfitting of the model.  The $C$ parameter in SVM can be thought\n",
    "of in the same way, though since it is a paramter on the cost function, smaller values of C represent penalizing the model for\n",
    "too much overfitting.\n",
    "\n",
    "Another way to think of $C$ in the context of SVM models is that the $C$ parameter is a positive value that controls the penalty\n",
    "for misclassified training examples.  A small $C$ parameter tells SVM to not worry so much about some misclassification errors, as\n",
    "long as the decision boundary is being maximized.  A large $C$ parameter, on the other hand$ tells the SVM optimization to try\n",
    "hard to correctly classify all the examples correctly.  As we have talked about before, at some point this can lead to overfitting,\n",
    "and though the model parameters of an overfitted model will classify well (or perfectly) our training set data, they will probably\n",
    "not perform well on unseen data if the model is overfitted.\n",
    "\n",
    "In the next cell, create a linear SVM classification model (using the svm class from `scikit-learn`).  Use $C = 1$ for the\n",
    "$C$ parameter, and plot the resulting decision boundary of the classifier on a plot with the original positive and negative\n",
    "examples plotted as '+' and 'o' symbols as you did in the first figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T16:17:26.802415Z",
     "start_time": "2018-10-19T16:17:26.234464Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a linear SVM classifier using scikit-learn\n",
    "\n",
    "# display the decision boundary for the coeficients\n",
    "\n",
    "# visualize the data points of the two categories\n",
    "\n",
    "# add the decision boundary line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you performed the previous task correctly, you should see that your SVM classifier with the $C$ parameter set to 1 chooses a decision boundary that basically ignores the outlier point, and the decision boundary appears to be directly in the center of the\n",
    "natural separation indicated by the gap between the positive and negative examples.\n",
    "\n",
    "In the next cell, repeat your previous work, but use a value of $C = 100$ for the $C$ parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T16:17:27.351555Z",
     "start_time": "2018-10-19T16:17:26.807626Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a linear SVM classifier using scikit-learn\n",
    "\n",
    "# display the decision boundary for the coeficients\n",
    "\n",
    "# visualize the data points of the two categories\n",
    "\n",
    "# add the decision boundary line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the effect that the large value of $C$ has on the decision boundary the model produces.  For learning models that use a\n",
    "$C$ parameter, which affects how much weight is given to the cost of the differences between the model and the correct result, high\n",
    "$C$ values will result in models that attempt to classify as much of the training data correctly as possible.  But, as we have\n",
    "discussed, these may lead to overfitted models, that have problems generalizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with Gaussian Kernels\n",
    "\n",
    "The SVM classifier from the `scikit-learn` library implements all of the typical and standard kernel functions that are used for\n",
    "this classifier.  However, the SVM classifier in `scikit-learn` will also let you specify your own kernel function, if you need\n",
    "some sort of special kernel measure of similarity.  In this part of our assignment, we are not going to make up a new kernel function, but you will implement the gaussian kernel function.  We will then use your implementation of the gaussian kernel\n",
    "to create an SVM classifier for a data set that is not linearly separable, and we will compare and show that your implementation\n",
    "of the gaussian kernel is correct by comparing the results to what are obtained by using `scikit-learn`s implementation of\n",
    "a gaussian kernel.\n",
    "\n",
    "To find non-linear decision boundaries with SVM, we need to first implement a Gaussian kernel. \n",
    "You can think of the Gaussian kernel as a similarity function that measures the “distance” between a pair of examples,\n",
    "$(x^{(i)} , x^{(j)} )$. The Gaussian kernel is also parameterized by a bandwidth parameter, $\\sigma$, which \n",
    "determines how fast the similarity metric decreases (to 0) as the examples are further apart.\n",
    "\n",
    "The gaussian kernel is defined as:\n",
    "\n",
    "$$\n",
    "K_{\\textrm{gaussian}}(x^{(i)}, x^{(j)}) = \\textrm{exp}(- \\frac{\\| x^{(i)} - x^{(j)} \\|^2}{2 \\sigma^2})\n",
    "= \\textrm{exp}(- \\frac{\\sum_{k=1}^{n} (x_k^{(i)} - x_k^{(j)})^2 }{2 \\sigma^2})\n",
    "$$\n",
    "\n",
    "Here recall that $x^{(i)}$ and $x^{(j)}$ represent two separate points, where usually one is an input and the other is a landmark\n",
    "location.  Both of these are $n$ dimensional vectors, where we have a measurement for each of the $n$ features of the data we\n",
    "are trying to model.  The top part of the fraction calculates norm, which is related to the distance, between the\n",
    "two points in $n$ dimensional space\n",
    "then squares this value.  As you can see on the right side part of the equation, we can calculate this norm by \n",
    "simply summing up the square of the differences of each of the dimensions.  Also remember that $\\sigma$ is simply a parameter\n",
    "that controls how fast the function falls to 0 as the distance between the two given locations is increased.\n",
    "\n",
    "In the next cell, create a function called gaussian_kernel that takes 2 n-dimensional (numpy) vectors as input.  It should\n",
    "calculate the gaussian similarity between these two points and return the similarity measure as a scalar floating point\n",
    "value.  Here is an example of using the function if you have it implemented correctly:\n",
    "\n",
    "```python\n",
    "\n",
    "sigma = 2.0\n",
    "xi = np.array([1, 0, -1, -3])\n",
    "xj = np.array([1, 0, -1, -3])\n",
    "print gaussian_kernel(xi, xj, sigma)\n",
    ">>> 1.0\n",
    "\n",
    "xi = np.array([1, 1, -1, -3])\n",
    "xj = np.array([2, 0, -1, -5])\n",
    "print gaussian_kernel(xi, xj, sigma)\n",
    ">>> 0.472366552741\n",
    "\n",
    "xi = np.array([1, 6,  2, -2])\n",
    "xj = np.array([5, 0, -1, -5])\n",
    "print gaussian_kernel(xi, xj, sigma)\n",
    ">>> 0.000158461325116\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T16:17:27.441312Z",
     "start_time": "2018-10-19T16:17:27.358156Z"
    }
   },
   "outputs": [],
   "source": [
    "# write your implementation of gaussian_kernel here\n",
    "\n",
    "sigma = 2.0\n",
    "xi = np.array([1, 0, -1, -3])\n",
    "xj = np.array([1, 0, -1, -3])\n",
    "print(gaussian_kernel(xi, xj, sigma))\n",
    "\n",
    "xi = np.array([1, 1, -1, -3])\n",
    "xj = np.array([2, 0, -1, -5])\n",
    "print(gaussian_kernel(xi, xj, sigma))\n",
    "\n",
    "xi = np.array([1, 6,  2, -2])\n",
    "xj = np.array([5, 0, -1, -5])\n",
    "print(gaussian_kernel(xi, xj, sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Dataset 2\n",
    "\n",
    "In the next cell we will load a new data set.  This data set still has 2 features ($n = 2$), but it is clearly not linearly\n",
    "separable.  As we did before, create a visualization of this data using '+' markers for the positive examples and 'o'\n",
    "markers for the negative examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T16:17:27.466771Z",
     "start_time": "2018-10-19T16:17:27.447219Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/assg-04-data2.csv', names=['x1', 'x2', 'y'])\n",
    "X = data[['x1', 'x2']].as_matrix()\n",
    "y = data.y.as_matrix()\n",
    "m,n = X.shape # m = number of training examples, n = number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T16:14:00.435988Z",
     "start_time": "2018-10-19T16:13:57.502520Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot the data to visualize it here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our Lecture 10 notebook, we showed an example of creating a non-linear SVM classifier using a gaussian kernel.  This was our\n",
    "last example in the lecture notebook.  In that example, by specifying the `gamma = 1.0` parameter when creating the SVM \n",
    "classifier, we were basically asking for a standard gaussian kernel.  \n",
    "\n",
    "However, if you examine the [scikit-learn SVM classifier documentation(http://scikit-learn.org/stable/modules/svm.html) \n",
    "and read about the radial basis functions (rbf), you will find that the rbf kernel functions have the following definition:\n",
    "\n",
    "$$\n",
    "K_{\\textrm{rbf}} = \\textrm{exp}(- \\gamma \\|x - x' \\|^2 )\n",
    "$$\n",
    "\n",
    "So to be precise, if you want to get exactly the gaussian kernel functions as were discussed in the video, where we have a $\\sigma$\n",
    "parameter which controls how fast the similarity decays between the 2 points to 0, we have the relationship between $\\gamma$\n",
    "and $\\sigma$ of the following:\n",
    "\n",
    "$$\n",
    "\\gamma = \\frac{1}{2 \\sigma^2}\n",
    "$$\n",
    "\n",
    "I have used the techniques discussed in the previous week 8 and found out that a value of $C = 1.0$ and a $\\sigma = 0.1$ works\n",
    "well for this second dataset to classify the data.  A $sigma$ of $0.1$ works out to $\\gamma = \\frac{1}{2 \\times 0.1^2} = 50$.\n",
    "In the next cell, create a SVM classifer for the second data set.  Use a `rbf` kernel function with a $\\sigma = 0.1$ (e.g.\n",
    "set the `gamma` parameter that scikit-learn uses to be 50).  After you have trained the classifier, visualize the decision\n",
    "boundary as we did in the lecture 10 notebook by creating a mesh of points to plot a contour of the boundary.  Use a step size of \n",
    "0.01.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T16:17:29.384695Z",
     "start_time": "2018-10-19T16:17:27.472949Z"
    }
   },
   "outputs": [],
   "source": [
    "# create and classifier and train it\n",
    "\n",
    "\n",
    "# create a mesh to plot in\n",
    "\n",
    "# Put the result into a color plot\n",
    "\n",
    "# plot the original data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets just demonstrate that the `rbf` kernel function with a `gamma` of 50 is the same as the gaussian kernel function we saw in\n",
    "our lecture notebook with a $\\sigma$ value of 0.1.  In the next cell I have created a wrapper function that will call your\n",
    "`gaussian_kernel` function you created previously.  This is the form of the function that `scikit-learn` expects when you want\n",
    "to specify your own version of the kernel function to use.  I have also shown how to create the `scikit-learn` `SVM` classifier\n",
    "specifying our user defined function for the kernel function.  Try training the classifier in the next cell using our own\n",
    "user defined kernel functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T16:22:26.812487Z",
     "start_time": "2018-10-19T16:17:29.389304Z"
    }
   },
   "outputs": [],
   "source": [
    "def gaussian_kernel_combos(Xi, Xj):\n",
    "    sigma = 0.1\n",
    "    m1, n = Xi.shape\n",
    "    m2, n = Xj.shape\n",
    "    res = np.zeros( (m1, m2) )\n",
    "    #print res.shape\n",
    "    for i, xi in enumerate(Xi):\n",
    "        for j, xj in enumerate(Xj):\n",
    "            similarity = gaussian_kernel(xi, xj, sigma)\n",
    "            res[i, j] = similarity\n",
    "    return res\n",
    "            \n",
    "# train your classifier using the provided kernel function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, plot again the decision boundary (using a h step size of 0.01) for this classifier using our user defined\n",
    "kernel functions.  Notice that this implementation is much slower.  However, if you examine the decision boundary that was \n",
    "produced,  you should see that we are getting basically the same model and decision boundary using the two ways of specifying\n",
    "the gaussian kernel function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T16:22:26.814659Z",
     "start_time": "2018-10-19T16:17:25.053Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# create a mesh to plot in\n",
    "\n",
    "# Put the result into a color plot\n",
    "\n",
    "# plot the original data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
