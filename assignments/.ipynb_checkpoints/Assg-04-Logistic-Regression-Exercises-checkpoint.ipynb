{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T21:14:53.564720Z",
     "start_time": "2018-08-25T21:14:52.673967Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (10, 8) # set default figure size, 8in by 6in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 03: Logistic Regression\n",
    "\n",
    "**Due Date:** Friday 10/4/2019 (by midnight)\n",
    "\n",
    "\n",
    "## Introduction \n",
    "\n",
    "In this exercise, you will implement logistic regression on a new data set.  We will then practice\n",
    "using the `NumPy` optimization methods in order to fit a set of parameters to this data set.  Thus\n",
    "you will be writing the cost and gradient functions needed by the `NumPy` optimization routines\n",
    "in order to search for the optimal theta parameters for the given data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "In this part of the exercise, you will build a logistic regression model to\n",
    "predict whether a student gets admitted into a university.\n",
    "\n",
    "Suppose that you are the administrator of a university department and\n",
    "you want to determine each applicant’s chance of admission based on their\n",
    "results on two exams. You have historical data from previous applicants\n",
    "that you can use as a training set for logistic regression. For each training\n",
    "example, you have the applicant’s scores on two exams and the admissions\n",
    "decision. \n",
    "\n",
    "Your task is to build a classification model that estimates an applicant’s\n",
    "probability of admission based the scores from those two exams.\n",
    "\n",
    "**Visualizing the Data**\n",
    "\n",
    "Before starting to implement any learning algorithm, it is always good to\n",
    "visualize the data if possible. In the next cell we have loaded the data\n",
    "set for you from the course repository into the standard x and y variables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T21:14:53.574839Z",
     "start_time": "2018-08-25T21:14:53.566873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n",
      "(100,)\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data/assg-03-data.csv', names=['exam1', 'exam2', 'admitted'])\n",
    "x = data[['exam1', 'exam2']].values\n",
    "y = data.admitted.values\n",
    "m = y.size\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file contains data from 100 students.  In `X` there are two input variables, the scores for Exam 1\n",
    "and Exam 2 that each student got.  In the `y` vector are 100 outputs, which are positive and negative\n",
    "results, which in this care are whether the student was admitted (a 1) or not (a 0) given their test\n",
    "scores (and presumably other admission materials).\n",
    "\n",
    "In the next cell, plot the Exam 1 and Exam 2 scores on the x and y axis of a scatter plot.  However,\n",
    "use yellow circles for the negative examples markers (not admitted, or where y==0), and use red\n",
    "triangles for the positive examples (admitted, or where y==1).  You will need to create an index\n",
    "for each set of positive and negative results, and perform two separate plot commands.  See our lecture\n",
    "notebooks for an example with a data set of doing this plot.  Your final figure should look exactly\n",
    "like this (don't forget to add x and y axis labels and a legend to the figure):\n",
    "\n",
    "<img src=\"files/figures/assg-03-plot-1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T21:14:53.670477Z",
     "start_time": "2018-08-25T21:14:53.576816Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot the visualization of the exam scores here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warmup Exercise: sigmoid function**\n",
    "\n",
    "Before you start with the actual cost and gradient functions, recall that the logistic regression\n",
    "hypothesis is defined as:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = g(\\theta^T x)\n",
    "$$\n",
    "\n",
    "where function $g()$ is the sigmoid function.  The sigmoid function is defined as:\n",
    "\n",
    "$$\n",
    "g(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "In the next cell, complete the function named `sigmoid` so it can be called by the rest of the cells\n",
    "in this assignment.  Your `sigmoid` function should be a vectorized function.  That is, it should be\n",
    "able to accept both scalar float values, as well as `NumPy` arrays of float values.  Recall that for \n",
    "negative values, the result of the sigmoid is close to 0, and for positive values, it is close to 1.\n",
    "For an input of $z = 0$ the output of the sigmoid function is exactly $0.5$.\n",
    "\n",
    "```python\n",
    "print sigmoid(-5.0)\n",
    ">>> 0.00669285092428\n",
    "print sigmoid(0.0)\n",
    ">>> 0.5\n",
    "print sigmoid(5.0)\n",
    ">>> 0.993307149076\n",
    "\n",
    "z = np.linspace(-5.0, 5.0, 11)\n",
    "print z\n",
    "print sigmoid(z)\n",
    ">>> [-5. -4. -3. -2. -1.  0.  1.  2.  3.  4.  5.]\n",
    ">>> [ 0.00669285  0.01798621  0.04742587  0.11920292  0.26894142  0.5\n",
    "      0.73105858  0.88079708  0.95257413  0.98201379  0.99330715]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T21:14:53.761942Z",
     "start_time": "2018-08-25T21:14:53.672616Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Calculate the sigmoid or logistic function on the values of z.  This is\n",
    "    a vectorized function, so z can be a single simple scalar float value, or it\n",
    "    can be a NumPy array of float values.  Any and all values given have their\n",
    "    sigmoid computed and returned as a result of calling this function.\n",
    "    \"\"\"\n",
    "    # your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T21:14:53.864336Z",
     "start_time": "2018-08-25T21:14:53.763815Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "[-5. -4. -3. -2. -1.  0.  1.  2.  3.  4.  5.]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(sigmoid(-5.0))\n",
    "print(sigmoid(0.0))\n",
    "print(sigmoid(5.0))\n",
    "z = np.linspace(-5.0, 5.0, 11)\n",
    "print(z)\n",
    "print(sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cost Function**\n",
    "\n",
    "As we mentioned, we are going to be using the `NumPy` optimize library in order to find optimized $\\theta$\n",
    "parameters for a logistic regression of our admission data classification task.  We showed examples\n",
    "of doing this in our Lecture notebooks this week.  In order to use something like BFGS optimization, we\n",
    "need to supply a function that computes the cost for the data given some set of $\\theta$ parameters, and\n",
    "also a function that calculates the gradients, or the partial derivatives, of the $\\theta$ parameters.\n",
    "\n",
    "First of all lets implement a function to compute the cost using our logistic cost function.  Recall\n",
    "that the logistic cost function is as follows:\n",
    "\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{m} \\big[ \\sum_{i=1}^m  y^{(i)} \\; \\textrm{log} (h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\; \\textrm{log} (1 - h_\\theta(x^{(i)})) \\big]\n",
    "$$\n",
    "\n",
    "Also recall that our hypothesis function for logistic regression is:\n",
    "\n",
    "$$\n",
    "h_{\\theta}(x) = g(\\theta^T x)\n",
    "$$\n",
    "\n",
    "Where $g(z)$ is the sigmoid or logistic function, which you should have just implemented in the previous\n",
    "cell.\n",
    "\n",
    "Notice that the cost function $J()$ is a function of the $\\theta$ parameters, as well as of the inputs\n",
    "$x$ and the correct outputs $y$ (which are all either 0 or 1 for this logistic classification task).\n",
    "In the next cell, implement the cost function for logistic regression.  I have given you the\n",
    "signature of the function, your task is to implement the logic to calculate the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T21:14:53.962056Z",
     "start_time": "2018-08-25T21:14:53.867307Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_logistic_cost(theta, X, y):\n",
    "    \"\"\"Compute the cost function for logistic regression.  \n",
    "    \n",
    "    Given a set of inputs X (we assume that the first column has been \n",
    "    initialized to 1's for the theta_0 parameter), and the correct \n",
    "    outputs for these inputs y, calculate the logistic regression cost\n",
    "    for the given input/outputs as defined by the theta parameters.\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    theta (numpy nx1 array) - An array of the set of theta parameters\n",
    "       to evaluate\n",
    "    X (numpy mxn array) - The example inputs, first column is expected\n",
    "       to be all 1's.\n",
    "    y (numpy m size array) - A vector of the correct outputs of length m\n",
    "       \n",
    "    Returns\n",
    "    -------\n",
    "    J (float) - The sum squared difference cost function for the given\n",
    "       theta parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # determine the number of training examples from the size of the correct outputs\n",
    "    m = len(y)\n",
    "    \n",
    "    # You need to return the following variable correctly\n",
    "    J = 0.0\n",
    "    \n",
    "    # ===== Your Code Here ======\n",
    "    # Instructions: Compute the cost of a particular choice of theta\n",
    "    # and return the resulting cost J\n",
    "    # Don't forget to use the sigmoid function you just created above when computing the\n",
    "    # hypothesis\n",
    "    \n",
    "    # we are taking the log of the hypothesis and the log of (1.0 - hypothesis).  However, the\n",
    "    # logarithm is undefined for the value 0, and it is possible that you can end up with\n",
    "    # a value close to 0 or 1 from the sigmoid function.  To avoid problems, I usually threshold\n",
    "    # the hypothesis values before taking the log of them.  For example, if I used matrix\n",
    "    # operations to calculate all of the hypothesis for all the inputs, and ended up with\n",
    "    # a NumPy array called hypothesis, I could use the following code to set all values\n",
    "    # that are close to 0, to some small epsilon instead, and likewise to find and set all\n",
    "    # values that are close to 1, and set them to be not quite so close to 1.\n",
    "    eps = 1e-12\n",
    "    hypothesis[hypothesis < eps] = eps\n",
    "    eps = 1.0 - 1e-12\n",
    "    hypothesis[hypothesis > eps] = eps\n",
    "\n",
    "    # feel free to implement the cost function as a loop or using matrix operations, you\n",
    "    # can do it either way.  \n",
    "\n",
    "    # return the calculated cost in the J variable\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use the same epsilon value that I used above to threshold any hypothesis values that are close\n",
    "to 0 or 1, you should get close to the following results when using your cost function:\n",
    "\n",
    "```python\n",
    "theta = np.zeros( (3, 1) )\n",
    "print compute_logistic_cost(theta, X, y)\n",
    ">>> 0.69314718056\n",
    "\n",
    "theta = np.array([[1.0],\n",
    "                  [1.0],\n",
    "                  [1.0]])\n",
    "print compute_logistic_cost(theta, X, y)\n",
    ">>> 11.0524172952\n",
    "\n",
    "theta = np.array([[0.1],\n",
    "                  [0.1],\n",
    "                  [0.1]])\n",
    "print compute_logistic_cost(theta, X, y)\n",
    ">>> 4.30613550295\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T21:14:54.095124Z",
     "start_time": "2018-08-25T21:14:53.963846Z"
    }
   },
   "outputs": [],
   "source": [
    "# Recall that for a model with inputs, we actually use 3 parameters, theta_0, theta_1 \n",
    "# and theta_2.  The inputs X need to have an initial column of all 1's for the theta_0\n",
    "# parameter.  So for our current data, X needs to be a 3xm shaped set of data, where \n",
    "# the first value in each column is 1.0, and the next value in each column is our raw inputs\n",
    "X = np.ones( (3, m) )\n",
    "X[1:,:] = x.T # the second column contains the raw inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T21:14:54.230282Z",
     "start_time": "2018-08-25T21:14:54.097041Z"
    }
   },
   "outputs": [],
   "source": [
    "theta = np.zeros( (3, 1) )\n",
    "print(compute_logistic_cost(theta, X, y))\n",
    "\n",
    "theta = np.array([[1.0],\n",
    "                  [1.0],\n",
    "                  [1.0]])\n",
    "print(compute_logistic_cost(theta, X, y))\n",
    "\n",
    "theta = np.array([[0.1],\n",
    "                  [0.1],\n",
    "                  [0.1]])\n",
    "print(compute_logistic_cost(theta, X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Function**\n",
    "\n",
    "As we showed in the lecture videos and notebooks, for the `NumPy` optimization functions we can also\n",
    "provide a function that determines the gradient (the partial derivative) with respect to each of\n",
    "the $\\theta$ parameters of our model. We presented the following equation for the gradient of the cost.\n",
    "Recall that the gradient of the cost is a vector of the same length as $\\theta$ where the $j^{th}$\n",
    "element (for $j = 0, 1, \\ldots, n)$ is defined as follows:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_j} =  \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}\n",
    "$$\n",
    "\n",
    "Note that while this gradient looks identical to the linear regression gradient, the formula is actually\n",
    "different because linear and logistic regression have different definitions of $h_\\theta(x)$.  So don't\n",
    "forget that you have to use the sigmoid function to calculate your hypothesis in the next cell.\n",
    "\n",
    "In the following cell, you will implement a function called `compute_logistic_cost_gradients` that\n",
    "will implement the calculation to find the gradients for each of the $\\theta$ parameters.  As\n",
    "we demonstrated in the lecture 05 notebook, this function needs to return a n+1 dimensional vector\n",
    "of the gradients for each of the $\\theta$ parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T21:14:54.311748Z",
     "start_time": "2018-08-25T21:14:54.232178Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_logistic_cost_gradients(theta, X, y):\n",
    "    \"\"\"Compute the gradients of the theta parameters for our logistic regression\n",
    "    cost function.\n",
    "    \n",
    "    Given a set of inputs X (we assume that the first column has been \n",
    "    initialized to 1's for the theta_0 parameter), and the correct \n",
    "    outputs for these inputs y, calculate the gradient of the cost function\n",
    "    with respect to each of the theta parameters.\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    theta (numpy nx1 array) - An array of the set of theta parameters\n",
    "       to evaluate\n",
    "    X (numpy mxn array) - The example inputs, first column is expected\n",
    "       to be all 1's.\n",
    "    y (numpy m size array) - A vector of the correct outputs of length m\n",
    "       \n",
    "    Returns\n",
    "    -------\n",
    "    gradients - A numpy n sized vector of the computed gradients.\n",
    "    \"\"\"\n",
    "\n",
    "    # determine the number of training examples from the size of the correct outputs\n",
    "    # and the number of parameters from the size of theta\n",
    "    m = len(y)\n",
    "    n = len(theta)\n",
    "    \n",
    "    # You need to return the following variable with the correctly calculated\n",
    "    # gradients of theta\n",
    "    gradients = np.zeros(n)\n",
    "\n",
    "    # your code goes here\n",
    "    \n",
    "    # return the numpy n sized vector of gradients\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some example gradient values you should get if you implement the above function correctly:\n",
    "\n",
    "```python\n",
    "theta = np.zeros( (3, 1) )\n",
    "print compute_logistic_cost_gradients(theta, X, y)\n",
    ">>> [ -0.1        -12.00921659 -11.26284221]\n",
    "\n",
    "theta = np.array([[1.0],\n",
    "                  [1.0],\n",
    "                  [1.0]])\n",
    "print compute_logistic_cost_gradients(theta, X, y)\n",
    ">>> [  0.4         20.81292044  21.84815684]\n",
    "\n",
    "theta = np.array([[0.1],\n",
    "                  [0.1],\n",
    "                  [0.1]])\n",
    "print compute_logistic_cost_gradients(theta, X, y)\n",
    ">>> [  0.39997223  20.81184964  21.84684953]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T21:14:54.413940Z",
     "start_time": "2018-08-25T21:14:54.313887Z"
    }
   },
   "outputs": [],
   "source": [
    "theta = np.zeros( (3, 1) )\n",
    "print(compute_logistic_cost_gradients(theta, X, y))\n",
    "\n",
    "theta = np.array([[1.0],\n",
    "                  [1.0],\n",
    "                  [1.0]])\n",
    "print(compute_logistic_cost_gradients(theta, X, y))\n",
    "\n",
    "theta = np.array([[0.1],\n",
    "                  [0.1],\n",
    "                  [0.1]])\n",
    "print(compute_logistic_cost_gradients(theta, X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Parameters using SciPy Optimize**\n",
    "\n",
    "In the previous assignment, you found the optimal parameters of a linear regression model by implementing\n",
    "the gradient descent algorithm youself by hand.  You wrote a cost function and calculated its gradient,\n",
    "then took a gradient descent step accordingly.\n",
    "\n",
    "This time, we will be using the `SciPy` optimization library routines, as we did in our lecture\n",
    "notebook, to use your cost and gradient functions to search for the optimal theta parameters for you.\n",
    "`SciPy`'s `minimize` function in the optimization library will find the theta parameters that minimize\n",
    "the cost function for the given data set.  This should correspond to the best decision boundary\n",
    "we can make for our given data, to differentiate between admit and no admit categories.\n",
    "\n",
    "`SciPy`'s `minimize` function is an optimization solver that finds the minimum of an unconstrained\n",
    "function.  For logistic regression, you want to optimize the cost function $J(\\theta)$ with\n",
    "parameters $\\theta$.\n",
    "\n",
    "Concretely, you are going to use `minimize` to find the best parameters $\\theta$ for the logistic\n",
    "regression cost function, given a fixed dataset (of X and y values).  You will pass to `minimize` the\n",
    "following inputs:\n",
    "\n",
    "- The initial values of the parameters we are trying to optimize\n",
    "- A function that, when given the training set and a particular set of $\\theta$ values, computes\n",
    "  the logistic regression cost of the dataset.\n",
    "- A function that, when given the training set and a particular set of $\\theta$ values, computes\n",
    "  the gradient of the cost with respect to $\\theta$ for the dataset.\n",
    "  \n",
    "In the following cell, use the example from the Lecture 5 notebook to correctly call the\n",
    "`minimize` function from `SciPy` using the cost and gradient functions you developed above.\n",
    "You should use an initial set of theta parameters of all 0's to start with.  Any of the\n",
    "optimization methods we demonstrated should work to find the optimal parameters (BFGS, CG,\n",
    "L-BFGS-B), though for me it looks like the CG solver is not converging correctly, so you probably\n",
    "need to use one of the BFGS methods.\n",
    "\n",
    "For me, the optimum cost is about 0.203498, and this optimum happens with the following theta values:\n",
    "\n",
    "```python\n",
    "Optimization terminated successfully.\n",
    "         Current function value: 0.203498\n",
    "         Iterations: 20\n",
    "         Function evaluations: 31\n",
    "         Gradient evaluations: 31\n",
    "[-25.16133596   0.20623176   0.2014716 ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Evaluating Logistic Regression **\n",
    "\n",
    "If you got the `minimize` optimization function to work correctly, you should have 3 values for your\n",
    "$\\theta$ parameters in the `res.x` member variable of your result.  Lets replot your first figure, but\n",
    "use these $\\theta$ parameters to visualize the decision boundary that was found with our logistic\n",
    "regression.\n",
    "\n",
    "Recall that the theta parameters represent a set of values that give us a decision boundary.  If\n",
    "we have the hypothesis:\n",
    "\n",
    "$h_\\theta(x) = g(\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2)$\n",
    "\n",
    "for some given values of theta, we will predict that $y = 1$ when:\n",
    "\n",
    "$\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 \\ge 0$\n",
    "\n",
    "Recall that the $\\theta$ parameters are simply scalar values, and we know the values we want to use\n",
    "for these 3 parameters from our previous cell.  Also recall that the decision boundary that divides\n",
    "our prediction between $y = 1$ and $y = 0$ occurs when the above equation is exactly 0:\n",
    "\n",
    "$\\theta_1 x_1 + \\theta_2 x_2 = - \\theta_0$\n",
    "\n",
    "We can solve this equation for $x_2$, for example:\n",
    "\n",
    "$x_2 = - \\frac{\\theta_0 + \\theta_1 x_1}{\\theta_2}$\n",
    "\n",
    "Given this and particular values for $\\theta$, we can then draw the decision boundary line\n",
    "on a figure by selecting some $x_1$ points and calculating the $x_2$ values.\n",
    "\n",
    "In the next cell, recreate our first figure showing all 100 student's data points, with markers \n",
    "coding for the admit/no admit categories.  Then add a blue line to represent the optimal decision\n",
    "boundary that was found from our previous cell optimization.  If you have done optimization and\n",
    "visualization correctly, your figure should look like this:\n",
    "\n",
    "<img src=\"files/figures/assg-03-plot-2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T21:14:54.552414Z",
     "start_time": "2018-08-25T21:14:54.415796Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot the exam score visualization figure again, along with the\n",
    "# decision boundary that was found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After learning the parameters, you can use the model to predict whether a particular student\n",
    "will be admitted or not.  For a student with an Exam 1 score of 45 and an Exam 2 score of 85, you \n",
    "should expect to see an admission probability of 0.776.\n",
    "\n",
    "Write a function in the next cell, called `model_predictions`.  This function should take\n",
    "an array of `theta` parameters, and a matrix like our X matrix, that represents exam scores (plus an\n",
    "additional first column of 1's).  This function will return a vector of the computed probabilities.\n",
    "\n",
    "For example, if I give the function the following set of new student scores, it will return the following\n",
    "predicted probability for the students:\n",
    "\n",
    "```python\n",
    "X_new = np.array([[ 1,  1,  1,  1],\n",
    "                  [45, 55, 75, 95],\n",
    "                  [85, 75, 35, 85]])\n",
    "theta = np.array([[-25.16133596],\n",
    "                  [0.20623176],\n",
    "                  [0.2014716]])\n",
    "\n",
    "print model_predictions(theta, X_new)\n",
    ">>> [[ 0.77629063  0.78444846  0.066456    0.99999042]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T21:14:54.636186Z",
     "start_time": "2018-08-25T21:14:54.554361Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_predictions(theta, X):\n",
    "    \"\"\"Given a set of theta parameters, and a set of inputs in X, calculate the models\n",
    "    prediction of admission.\n",
    "    \n",
    "    theta (numpy nx1 array) - An array of the set of theta parameters\n",
    "       to evaluate\n",
    "    X (numpy mxn array) - The example inputs, first column is expected\n",
    "       to be all 1's.\n",
    "    \"\"\"\n",
    "    # put your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T21:14:54.728566Z",
     "start_time": "2018-08-25T21:14:54.638012Z"
    }
   },
   "outputs": [],
   "source": [
    "X_new = np.array([[ 1,  1,  1,  1],\n",
    "                  [45, 55, 75, 95],\n",
    "                  [85, 75, 35, 85]])\n",
    "theta = np.array([[-25.16133596],\n",
    "                  [0.20623176],\n",
    "                  [0.2014716]])\n",
    "\n",
    "print(model_predictions(theta, X_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to evaluate the quality of the parameters we have found is to see how well the learned\n",
    "model predicts on our training set.  You can easily use the results from your `model_predictions`\n",
    "function to get predicted probabilities for all of our original `X` data in the training set.  If\n",
    "we threshold these results at the 0.5 level, you should end up with an array of 100 1s and 0's (or true\n",
    "and false).\n",
    "Using this array, and the array `y` containing the correct answers, compute the percentage of\n",
    "inputs that our model gets correct from our training data set.  From the previous figure, you \n",
    "should be able to see that the model decision boundary gets about 11 of the inputs in the wrong category,\n",
    "given an 89% accuracy.  In the next cell, compute the accuracy of our logistic model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
