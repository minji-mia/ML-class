{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T16:45:13.947768Z",
     "start_time": "2018-09-12T16:45:13.693559Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (8, 6) # set default figure size, 8in by 6in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video W4 01: Non-linear Hypothesis\n",
    "\n",
    "[YouTube Video Link](https://www.youtube.com/watch?v=mcnvIWDnPns&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW&index=44)\n",
    "\n",
    "We have already seen some linear regression and logistic classification problems that would not\n",
    "be easily modeled using only linear assumptions.  A couple of times in the companion videos, it was\n",
    "mentioned that the techniques we developed do actually extend easily to using non-linear hypothesis.\n",
    "For example, we could expand a given set of features to include all of the quadratic combinations\n",
    "(raise to the second power).\n",
    "This can work ok if we only have a few 10s to 100s of original features, though we might have to expand\n",
    "beyond quadratic and even look at third power, fourth power combinations, etc.  For quadratic \n",
    "features, as the video mentions, the number of inputs will grow with the square of the number of\n",
    "original features, so a problem with 100 original inputs, would require about $10,000 / 2$ \n",
    "quadratic combinations, and higher order powers grow even faster.  Thus for problems of more than\n",
    "100 or so original inputs, it quickly becomes infeasable to model and solve these problems using\n",
    "standard regression methods.\n",
    "\n",
    "So these next two weeks we will be looking at a different learning method, neural networks.  Neural\n",
    "networks are able to create models or solutions using non-linear combinations of features, without\n",
    "the problems we just saw with the combinotorial explosion of combinations of the features.\n",
    "\n",
    "# Video W4 02: Neurons and the Brain\n",
    "\n",
    "[YouTube Video Link](https://www.youtube.com/watch?v=Nx3HVwg2uGA&index=45&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW)\n",
    "\n",
    "# Video W4 03: Model Representation I\n",
    "\n",
    "[YouTube Video Link](https://www.youtube.com/watch?v=wnSol2JRZeY&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW&index=46)\n",
    "\n",
    "Neural networks use a \"hypothesis\" or model for individual units of computation that is the same\n",
    "as we have already seen.\n",
    "\n",
    "<img src=\"../../figures/nn-model-logistic-unit.png\">\n",
    "\n",
    "What happens in a neural network is that the inputs $x_1, x_2, x_3$ are multiplied by what are known as\n",
    "weights $w_1, w_2, w_3$, which are associated with the input wires in the diagram.  The weights are\n",
    "the same as the $\\theta_1, \\theta_2, \\theta_3$ parameters we have been using in the previous lectures\n",
    "(but by convention they are usually referred to as weights rather than parameters in the\n",
    "context of neural networks).  Inputs and weight\n",
    "parameters are multiplied and then summed together.  Then the output of the neuarl network unit\n",
    "is passed through a non-linear function.  The most common output function to use is the logistic\n",
    "(sigmoid) function, the same as we used for logistic regression:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^Tx}}\n",
    "$$\n",
    "\n",
    "But what makes a neural network different from a logistic regression, is that we organize the\n",
    "computation of the hypothesis into many small regression/calculations, and we combine multiple of\n",
    "these using 2 or more layers of the network.\n",
    "\n",
    "# Video W4 04: Model Representation II\n",
    "\n",
    "[YouTube Video Link](https://www.youtube.com/watch?v=vuhueI_7324&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW&index=47)\n",
    "\n",
    "To summarize, this video looks in detail at the equations we will be using for a vectorized implementation of a\n",
    "neural network.  We use the following example neural network with 3 inputs, 3 units in the hidden layer, and 1 output:\n",
    "\n",
    "<img src=\"../../figures/nn-example-model.png\">\n",
    "\n",
    "The following equations give us statements for calculating what is known as the feed forward activation of the network:\n",
    "\n",
    "$$\n",
    "a_1^{(2)} = g\\big( \\Theta_{10}^{(1)} x_0 + \\Theta_{11}^{(1)} x_1 + \\Theta_{12}^{(1)} x_2 + \\Theta_{13}^{(1)} x_3 \\big)\n",
    "$$\n",
    "\n",
    "$$\n",
    "a_2^{(2)} = g\\big( \\Theta_{20}^{(1)} x_0 + \\Theta_{21}^{(1)} x_1 + \\Theta_{22}^{(1)} x_2 + \\Theta_{23}^{(1)} x_3 \\big)\n",
    "$$\n",
    "\n",
    "$$\n",
    "a_3^{(2)} = g\\big( \\Theta_{30}^{(1)} x_0 + \\Theta_{31}^{(1)} x_1 + \\Theta_{32}^{(1)} x_2 + \\Theta_{33}^{(1)} x_3 \\big)\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_\\Theta(x) = a_1^{(3)} = g\\big( \\Theta_{10}^{(2)} a_0^{(2)} + \\Theta_{11}^{(1)} a_1^{(2)} + \\Theta_{12}^{(1)} a_2^{(2)} + \\Theta_{13}^{(1)} a_3^{(2)} \\big)\n",
    "$$\n",
    "\n",
    "There are a lot of equations, but this is mostly just notation.  Be sure that you understand the basic ideas of the notation.  \n",
    "The $a_1^{(2)}$ are the activations (the outputs) of the units in the hidden layer, or layer 2.  Layer 1 refers to the raw inputs\n",
    "used for the problem, thus the first layer of actual computing units is layer 2.  The output layer in this model network\n",
    "can also be referred to as layer 3.  Notice that in all of the equations, the superscript does not refer to raising the value\n",
    "to a power, they simply refer to the layer numbers of the units in our network.  The $\\Theta_{10}^{(1)}$ values represent\n",
    "the parameter of the model we will be trying to learn.  As mentioned previously, in most neural network literature these\n",
    "are refered to as the network weights, but in our companion videos we will continue to refer to these parameters as $\\Theta$.\n",
    "Notice that, as with our previous linear and logistic regression, we have a bias unit (not shown in figure) named $X_0$.\n",
    "This bias unit will always have a value of $1.0$, but the weight from the bias unit can be set to any valid value.  Likewise,\n",
    "each layer in such a network will also have a bias unit, for example in our equations the output layer uses $a_0^{(2)}$\n",
    "which is a bias unit which will always have a value of $1.0$ as well.  \n",
    "\n",
    "Also make sure you understand the notation of\n",
    "the $\\Theta_{12}^{(1)}$ parameters.  The superscript simply means this is a weight between the input layer 1 and the hidden\n",
    "layer 2 (or you can think of it as the first set or layer of weights).  The subscript means this is the weight from input\n",
    "$x_2$ to the layer 2 unit 1.  Many other texts on neural networks would use a comman, like this $\\Theta_{i,j}^{(1)}$.\n",
    "Also notice that the order is reversed from what you might expect, the previous parameter would be read to mean that\n",
    "this is the weight coming from unit $j$ in layer 1, going to unit $i$ in layer 2.  The reason for switching the order of these\n",
    "is to make the definitions of the linear algebra operations on matrices representing the networks weights straightforward.\n",
    "\n",
    "\n",
    "Also as noted, we often refer to the set of $\\Theta$ parameters times the inputs/activations by the values $z_1^{(2)}.  For example\n",
    "\n",
    "$$\n",
    "z_1^{(2)} = \\Theta_{10}^{(1)} x_0 + \\Theta_{11}^{(1)} x_1 + \\Theta_{12}^{(1)} x_2 + \\Theta_{13}^{(1)} x_3\n",
    "$$\n",
    "\n",
    "With this definition, the activation of unit 1 in layer 2 becomes a function of:\n",
    "\n",
    "$$\n",
    "a_1^{(2)} = g(z_1^{(2)})\n",
    "$$\n",
    "\n",
    "where you should recall that $g()$ represents our logistic or sigmoid function.  Given this, we can calculate all of the $z$\n",
    "values for a layer using a single matrix operation, and all of the activations for a layer where the $g()$ function represents\n",
    "taking the elementwise logistic function of each of the values in the $z$ vector:\n",
    "\n",
    "$$\n",
    "z_1^{(2)} = \\Theta^{(1)} \\cdot x\n",
    "$$\n",
    "$$\n",
    "a^{(2)} = g(z^{(2)})\n",
    "$$\n",
    "\n",
    "You should realize that the $\\Theta$ represents a $3 \\times 4$ matrix (4 because of the bias unit input)\n",
    "in the previous equations, and that the multiplication\n",
    "shown is a $3 \\times 4$ matrix multiplication by a $4 \\times 1$ vector.  And as discussed in the vector, this results in\n",
    "3 activation values, but we add a bias activation value for the hidden layer, thus getting a $1 \\times 4$ sized \n",
    "matrix.\n",
    "\n",
    "This forward propagation of activation should remind you strongly of our previous logistic regression models.  Each unit in\n",
    "a neural network is computing a single case of logistic regression.  However, when we have multiple layers in a neural network,\n",
    "later layers are computing logistic regression on a set of features that the neural network has learned.\n",
    "\n",
    "How we learn the set of $\\Theta$ parameter weights in order to automatically learn whatever features work well to solve\n",
    "the problem will be discussed next week, when we look at the backpropagation algorithm for trying to optimize the parameters\n",
    "for a network to learn some model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video W4 05: Examples and Intuitions I\n",
    "\n",
    "[YouTube Video Link](https://www.youtube.com/watch?v=BhWlHvjEn3s&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW&index=48)\n",
    "\n",
    "**Simple Example: AND**\n",
    "\n",
    "Lets first look at the network to compute the **AND** function.\n",
    "\n",
    "$$\n",
    "h_{\\Theta}(x) = g \\big( -30 x_0 + 20 x_1 + 20 x_2 \\big)\n",
    "$$\n",
    "\n",
    "Here we have 2 inputs, and 1 bias unit.  This gives us a $1 \\times 3$ matrix of `Theta` parameters.  We can define a matrix of\n",
    "the inputs and the `Theta` parameters like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T16:45:13.953281Z",
     "start_time": "2018-09-12T16:45:13.949148Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "# parameters for AND function\n",
    "Theta = np.array([[-30.0, 20.0, 20.0]]) # a 1x3 matrix\n",
    "print(Theta.shape)\n",
    "\n",
    "x = np.array([1.0, 0.0, 0.0]) # a 3x1 vector, x_0 is our bias unit, and the next two values are x_1,x_2\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we will redefine our logistic function once again, and we will use matrix operations to compute the $z$\n",
    "value and get our final output for the previous set of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T16:45:13.966084Z",
     "start_time": "2018-09-12T16:45:13.954421Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-30.]\n",
      "[9.35762297e-14]\n"
     ]
    }
   ],
   "source": [
    "def g(z):\n",
    "    \"\"\"The logistic or sigmoid function, given a scalar value, or a numpy array of values in z\n",
    "    return the sigmoid of all values in z as our result.\n",
    "    \"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "z = np.dot(Theta, x)\n",
    "print(z)\n",
    "print(g(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to recreate the whole **AND** binary table, we can do the previous on all possible inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T16:45:13.975083Z",
     "start_time": "2018-09-12T16:45:13.967176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 1.]\n",
      " [1. 1. 0.]\n",
      " [1. 1. 1.]]\n",
      "(4, 3)\n"
     ]
    }
   ],
   "source": [
    "# our basic set of inputs\n",
    "Xraw = np.array([[0.0, 0.0],\n",
    "              [0.0, 1.0],\n",
    "              [1.0, 0.0],\n",
    "              [1.0, 1.0]])\n",
    "(rows, cols) = Xraw.shape\n",
    "print(rows, cols)\n",
    "\n",
    "# add in a column of 1.0s for the bias units\n",
    "X = np.ones((rows, cols+1))\n",
    "X[:, 1:] = Xraw\n",
    "print(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T16:45:13.982187Z",
     "start_time": "2018-09-12T16:45:13.976132Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_1 x_2 |     z |       g(z) | output\n",
      "  0   0 | -30.0 | 0.00000000 | 0.0\n",
      "  0   1 | -10.0 | 0.00004540 | 0.0\n",
      "  1   0 | -10.0 | 0.00004540 | 0.0\n",
      "  1   1 |  10.0 | 0.99995460 | 1.0\n"
     ]
    }
   ],
   "source": [
    "def threshold(z):\n",
    "    \"\"\"Given a value or array of values, perform a threshold at 0.5\"\"\"\n",
    "    return np.where(z >= 0.5, np.ones(z.shape), np.zeros(z.shape))\n",
    "    \n",
    "print(\"x_1 x_2 |     z |       g(z) | output\")\n",
    "for x in X:\n",
    "    z = np.dot(Theta, x)\n",
    "    #print x[1:], g(z), threshold(g(z))\n",
    "    print(\"%3d %3d | %5.1f | %0.8f | %0.1f\" % (x[1], x[2], z, g(z), threshold(g(z))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple Example: OR function**\n",
    "\n",
    "Here is the `Theta` parameters for the **OR** function, and the truth table results for this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T16:45:14.008631Z",
     "start_time": "2018-09-12T16:45:14.003106Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_1 x_2 |     z |       g(z) | output\n",
      "  0   0 | -10.0 | 0.00004540 | 0.0\n",
      "  0   1 |  10.0 | 0.99995460 | 1.0\n",
      "  1   0 |  10.0 | 0.99995460 | 1.0\n",
      "  1   1 |  30.0 | 1.00000000 | 1.0\n"
     ]
    }
   ],
   "source": [
    "# parameters for OR function\n",
    "Theta = np.array([[-10.0, 20.0, 20.0]]) # a 1x3 matrix\n",
    "\n",
    "print(\"x_1 x_2 |     z |       g(z) | output\")\n",
    "for x in X:\n",
    "    z = np.dot(Theta, x)\n",
    "    #print x[1:], g(z), threshold(g(z))\n",
    "    print(\"%3d %3d | %5.1f | %0.8f | %0.1f\" % (x[1], x[2], z, g(z), threshold(g(z))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video W4 06: Examples and Intuitions II\n",
    "\n",
    "[YouTube Video Link](https://www.youtube.com/watch?v=QZqmNpEyiKI&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW&index=49)\n",
    "\n",
    "**NOT or Negation**\n",
    "\n",
    "The example **NOT** function from the video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T16:45:14.051893Z",
     "start_time": "2018-09-12T16:45:14.042592Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_1 |     z |       g(z) | output\n",
      "  0 |  10.0 | 0.99995460 | 1.0\n",
      "  1 | -10.0 | 0.00004540 | 0.0\n"
     ]
    }
   ],
   "source": [
    "# parameters for the not function\n",
    "Theta = np.array([[10.0, -20.0]]) # a 1x2 matrix\n",
    "\n",
    "# our basic set of inputs\n",
    "Xraw = np.array([[0.0],\n",
    "                 [1.0]])\n",
    "(rows, cols) = Xraw.shape\n",
    "\n",
    "# add in a column of 1.0s for the bias units\n",
    "X = np.ones((rows, cols+1))\n",
    "X[:, 1:] = Xraw\n",
    "\n",
    "print(\"x_1 |     z |       g(z) | output\")\n",
    "for x in X:\n",
    "    z = np.dot(Theta, x)\n",
    "    #print x[1:], g(z), threshold(g(z))\n",
    "    print(\"%3d | %5.1f | %0.8f | %0.1f\" % (x[1], z, g(z), threshold(g(z))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Putting it together: $x_1 \\; \\textrm{XNOR} \\; x_2$**\n",
    "\n",
    "Before looking at the next cell you might want to tray putting together the code from the previous cells to build the network\n",
    "shown in the middle part of this video.  Notice that in this network we now have 2 layers, so we will need to set of \n",
    "`Theta` parameters, those for the weights from the inputs to the first activation layer, then those for the weights from\n",
    "these intermediate activations to our final output.  We will then need to calculate the final output using a 2 step process.\n",
    "\n",
    "Lets start by defining our two set of `Theta` layer weights, and our original set of inputs we need for our binary\n",
    "logical functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T16:45:14.084402Z",
     "start_time": "2018-09-12T16:45:14.079758Z"
    }
   },
   "outputs": [],
   "source": [
    "# our basic set of inputs\n",
    "Xraw = np.array([[0.0, 0.0],\n",
    "              [0.0, 1.0],\n",
    "              [1.0, 0.0],\n",
    "              [1.0, 1.0]])\n",
    "(rows, cols) = Xraw.shape\n",
    "\n",
    "# add in a column of 1.0s for the bias units\n",
    "X = np.ones((rows, cols+1))\n",
    "X[:, 1:] = Xraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T16:45:14.090840Z",
     "start_time": "2018-09-12T16:45:14.085662Z"
    }
   },
   "outputs": [],
   "source": [
    "# weights from layer 1 to layer 2, a 2 rows by 3 column matrix.  First column is from the bias unit\n",
    "# first row represents our simple x_1 AND x_2 feature\n",
    "# second row represnts the (NOT x_1) AND (NOT x_2)\n",
    "Theta1 = np.array([[-30.0,  20.0,  20.0],\n",
    "                   [ 10.0, -20.0, -20.0]])\n",
    "\n",
    "# weights from layer 2 to layer 3, a 1 row by 3 column matrix, First column will be from the bias unit\n",
    "# these weights represent a simple OR function\n",
    "Theta2 = np.array([[-10.0,  20.0,  20.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the final output, we need to perform two steps, compute the output activations for layer 2, then compute the final\n",
    "output activation of layer 3.  This is the basis of feed forward activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T16:45:14.127520Z",
     "start_time": "2018-09-12T16:45:14.118129Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0.]\n",
      "[-30.  10.]\n",
      "[9.35762297e-14 9.99954602e-01]\n",
      "[9.99909204]\n",
      "[0.99995456]\n"
     ]
    }
   ],
   "source": [
    "x = X[0] # select one of the inputs as an example, you can use any input X[0] to X[3] to test out the forward activation\n",
    "print(x)\n",
    "\n",
    "z2 = np.dot(Theta1, x)\n",
    "a2 = g(z2)\n",
    "print(z2)\n",
    "print(a2)\n",
    "\n",
    "# we need to add in a bias activation to the 2 activations we calculated\n",
    "a2_bias = np.ones( (3,))\n",
    "a2_bias[1:] = a2\n",
    "\n",
    "z3 = np.dot(Theta2, a2_bias)\n",
    "a3 = g(z3)\n",
    "print(z3)\n",
    "print(a3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should try out the previous cell for all the inputs and make sure you understand what is happenning.  \n",
    "\n",
    "Lets write a simple little function that will take two layers of `Theta` weights and some initial inputs, and compute\n",
    "the outputs of the network using the feed forward activation method we just demonstrated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T16:45:14.158934Z",
     "start_time": "2018-09-12T16:45:14.151480Z"
    }
   },
   "outputs": [],
   "source": [
    "def feedforward(x, Theta1, Theta2):\n",
    "    \"\"\"Given sets of Theta parameter weights from layer1 to layer2 and from layer2 to layer3\n",
    "    of a 3 layer network, compute the final feed forward activation for a given set of inputs.\n",
    "    \"\"\"\n",
    "    # activations of layer 2\n",
    "    z2 = np.dot(Theta1, x)\n",
    "    a2 = g(z2)\n",
    "    \n",
    "    # add a bias column to our activations\n",
    "    a2_bias = np.ones( (a2.size+1, ) )\n",
    "    a2_bias[1:] = a2\n",
    "    \n",
    "    # activations of layer 3\n",
    "    z3 = np.dot(Theta2, a2_bias)\n",
    "    a3 = g(z3)\n",
    "    \n",
    "    return a2, a3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our feed forward function, we can compute our complete **XNOR** truth table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T16:45:14.183843Z",
     "start_time": "2018-09-12T16:45:14.179090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_0 x_1 | a_1^2 a_2^2 | a_1^3\n",
      "  0   0 |   0.0   1.0 |   1.0\n",
      "  0   1 |   0.0   0.0 |   0.0\n",
      "  1   0 |   0.0   0.0 |   0.0\n",
      "  1   1 |   1.0   0.0 |   1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"x_0 x_1 | a_1^2 a_2^2 | a_1^3\")\n",
    "for x in X:\n",
    "    a2, a3 = feedforward(x, Theta1, Theta2)\n",
    "    print(\"%3d %3d | %5.1f %5.1f | %5.1f\" % (x[1], x[2], a2[0], a2[1], a3[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video W4 07: Multiclass Classification\n",
    "\n",
    "[YouTube Video Link](https://www.youtube.com/watch?v=HzpptanxP6A&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW&index=50)\n",
    "\n",
    "All of our previous simple logic function networks only had a single unit in the final output layer.  However, just as in\n",
    "our hidden layers we often had more than one activation unit, we can also have more than 1 output activation for our final\n",
    "output layer.  This can be used to learn multiclass classification problems. \n",
    "\n",
    "As discussed in the video, the most common way to do this is that if we have $N$ classes we want to learn in a classification\n",
    "problem using a neural network, we will build a network with $N$ units on the output layer.  We will then train it so that\n",
    "the correct outputs have only one of these units active for the correct class, and all other units are trained to output 0 for\n",
    "members of that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-12T16:45:50.512205Z",
     "start_time": "2018-09-12T16:45:50.442801Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Module   Versions\n",
      "--------------------   ------------------------------------------------------------\n",
      "         matplotlib:   ['3.3.0']\n",
      "              numpy:   ['1.18.5']\n",
      "             pandas:   ['1.0.5']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../src\") # add our class modules to the system PYTHON_PATH\n",
    "\n",
    "from ml_python_class.custom_funcs import version_information\n",
    "version_information()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3-datasci",
   "language": "python",
   "name": "python3-datasci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
