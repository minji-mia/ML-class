{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports usually needed\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.animation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sbn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries more specific to this lecture notebook\n",
    "import os.path\n",
    "import sys\n",
    "sys.path.append('../../src')\n",
    "from ml_python_class.config import DATA_DIR\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook wide settings to make plots more readable and visually better to understand\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "#%matplotlib widget\n",
    "#%matplotlib inline\n",
    "\n",
    "plt.rc('axes', labelsize=14)\n",
    "plt.rc('xtick', labelsize=12)\n",
    "plt.rc('ytick', labelsize=12)\n",
    "plt.rc('figure', titlesize=18)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rcParams['figure.figsize'] = (12.0, 8.0) # default figure size if not specified in plot\n",
    "plt.style.use('seaborn-darkgrid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classification\n",
    "\n",
    "This notebook is based on the following tutorials:\n",
    "\n",
    "- https://www.datacamp.com/community/tutorials/naive-bayes-scikit-learn\n",
    "\n",
    "Suppose you are a product manager, you want to classify customer reviews in positive and negative classes.\n",
    "Or As a loan manager, you want to identify which loan applicants are safe or risky? As a healthcare analyst,\n",
    "you want to predict which patients can suffer from diabetes disease. All the examples are the same kind of \n",
    "problem: a classification task of reviews or other customer features.\n",
    "\n",
    "Naive Bayes is the most straightforward and fast classification algorithm, which is suitable for a large chunk of\n",
    "data. Naive Bayes classifiers have been successfully used in many applications such as spam filtering, text classification,\n",
    "sentiment analysis, and recommender systems. It uses Bayes theorem of probability for prediction of unknown class.\n",
    "\n",
    "In this lecture notebook, you are going to learn about all of the following:\n",
    "\n",
    "\n",
    "- Classification Workflow\n",
    "- What is Naive Bayes classifier?\n",
    "- How Naive Bayes classifier works?\n",
    "- Classifier building in Scikit-learn\n",
    "- Zero Probability Problem\n",
    "- It's advantages and disadvantages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Workflow\n",
    "\n",
    "Whenever you perform classification, the first step is to understand the problem and identify potential features and the target label.\n",
    "Features are those characteristics or attributes which affect the results of the label. For example, in the case of a loan distribution,\n",
    "bank managers identify the customer’s occupation, income, age, location, previous loan history, transaction history, credit score, etc.\n",
    "These characteristics are known as features which help the model classify customers.\n",
    "\n",
    "The classification has two phases, a learning phase, and the evaluation phase. In the learning phase, the classifier trains its model\n",
    "on a given dataset.  And in the evaluation phase, it tests the classifier performance. Performance is evaluated on the basis of\n",
    "various parameters such as accuracy, error, precision, and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Naive Bayes Classifier?\n",
    "\n",
    "Naive Bayes is a statistical classification technique based on Bayes Theorem. It is one of the simplest supervised learning algorithms.\n",
    "Naive Bayes classifier is a fast, and mostly accurate and reliable algorithm. Naive Bayes classifiers have good accuracy and speed on large datasets.\n",
    "\n",
    "The Naive Bayes classifier assumes that the effect of a particular feature in a class is independent of other features. For example,\n",
    "a loan applicant is desirable or not depending on his/her income, previous loan and transaction history, age, and location. Even\n",
    "if these features are interdependent, these features are still considered independently by a naive Bayes classifier.\n",
    "This assumption simplifies computation, and that's why it is considered naive. This assumption is called class conditional independence.\n",
    "\n",
    "The standard specification of the conditional probability at the hear of Bayesien classification is given as:\n",
    "\n",
    "\\begin{equation}\n",
    "P(h|D) = \\frac{P(D|h) P(h)}{P(D)}\n",
    "\\end{equation}\n",
    "\n",
    "Where\n",
    "\n",
    "- $P(h)$: the probability of hypothesis $h$ being true (regardless of the evidence or data $D$).  This is known as the prior probability of $h$.\n",
    "- $P(D)$: the probability of the data (regardless of the hypothesis).  This is known as the prior probability.\n",
    "- $P(h|D)$: the probability of hypothesis $h$ given the data $D$.  This is known as the posterior probability.\n",
    "- $P(D|h)$: the probability of data $D$ given that the hypothesis $h$ was true.  This is known as posterior probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Naive Bayes Classifier Works\n",
    "\n",
    "Let’s understand how Naive Bayes works through an example. We will use an example of weather\n",
    "conditions and playing sports. We want to calculate the probability of playing sports given weather\n",
    "conditions. Now, you need to classify whether players will play or not, based on the weather condition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Approach (In case of a single feature)\n",
    "\n",
    "Naive Bayes classifier calculates the probability of an event in the following steps:\n",
    "\n",
    "1. Calculate the prior probability for the given class labels\n",
    "2. Find likelihood probability with each attribute for each class\n",
    "3. Put these values in Bayes formula and calculate posterior probabilities\n",
    "4. See which class has a higher probability, given the input belongs to the higher probability class\n",
    "\n",
    "For simplifying prior and posterior probability calculation we can use the two tables: frequency and likelihood tables.\n",
    "Both of these tables will help you to calculate the prior and posterior probability. The Frequency table contains the occurrence of\n",
    "labels for all features. There are two likelihood tables. Likelihood Table 1 is showing prior probabilities of labels and\n",
    "Likelihood Table 2 is showing the posterior probability.\n",
    "\n",
    "![naive Bayes figure 1](../../figures/naive-bayes-1.png)\n",
    "\n",
    "Now suppose you want to calculate the probability of playing when the weather is overcast.\n",
    "\n",
    "Probability of playing:\n",
    "\n",
    "\\begin{equation}\n",
    "P(Yes | Overcast) = \\frac{P(Overcast | Yes) P(Yes)}{P(Overcast)}\n",
    "\\end{equation}\n",
    "\n",
    "Besides the probabilities shown in our tables, we also have the following prior probabilities\n",
    "\n",
    "\\begin{equation}\n",
    "P(Overcast) = \\frac{4}{14} = 0.29 \\\\\n",
    "P(Yes) = \\frac{9}{14} = 0.64\n",
    "\\end{equation}\n",
    "\n",
    "The posterior probability is (from Likelihood Table 2):\n",
    "\n",
    "\\begin{equation}\n",
    "P(Overcast | Yes) = \\frac{4}{9} = 0.44\n",
    "\\end{equation}\n",
    "\n",
    "And finally we can calculate what we really want, the probability of playing when the whether is overcast\n",
    "\n",
    "\\begin{equation}\n",
    "P(Yes | Overcast) = \\frac{0.44 \\times 0.64}{0.29} = 0.97\n",
    "\\end{equation}\n",
    "\n",
    "We can also calculate the probability of not playing given that the sky is overcast.  Though\n",
    "here, because we always made the decision to play (given this data) on overcast days, the \n",
    "posterior is 0, and thus the conditional probability ends up with a probability of 0:\n",
    "\n",
    "Probability of not playing:\n",
    "\n",
    "\\begin{equation}\n",
    "P(Overcast) = \\frac{4}{14} = 0.29 \\\\\n",
    "P(No) = \\frac{5}{14} = 0.36\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "P(Overcast | No) = \\frac{0}{9} = 0\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "P(No | Overcast) = \\frac{0 \\times 0.36}{0.29} = 0\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in conclusion here.  Given information that the whether is overcast, we calculate the conditional probability of a Yes or No decision to\n",
    "play as shown. And since the condition probability of a Yes desicion is higher, we would determine that the most likely decision output for\n",
    "a new overcast data is Yes, we will play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Approach (In case of multiple features)\n",
    "\n",
    "\n",
    "So suppose we want to calculate the probability of playing when the whether is overcast and the temperature is mild:\n",
    "\n",
    "![naive Bayes figure 2](../../figures/naive-bayes-2.png)\n",
    "\n",
    "Probability of Playing:\n",
    "\n",
    "\\begin{equation}\n",
    "P(Yes | Weather=Overcast, Temp=Mild) = \\frac{P(Weather=Overcast, Temp=Mild | Yes) P(Yes)}{P(Weather=Overcast, Temp=Mild)}\n",
    "\\end{equation}\n",
    "\n",
    "Where by step 3, we can multiple conditional probabilities to get\n",
    "\n",
    "\\begin{equation}\n",
    "P(Weather=Overcast, Temp=Mild | Yes) = P(Overcast | Yes) P(Mild | Yes)\n",
    "\\end{equation}\n",
    "\n",
    "From the table above, we can determine all of the values we need to determine probability of playing and of not playing when weather is overcast and the temperature\n",
    "is mild.\n",
    "\n",
    "\\begin{equation}\n",
    "P(Weather=Overcast, Temp=Mild) = \\frac{1}{14} = 0.07 \\\\\n",
    "P(Yes) = \\frac{9}{14} = 0.64 \\\\\n",
    "P(Weather=Overcast, Temp=Mild | Yes) = \\frac{1}{9} = 0.1111 \\\\\n",
    "P(Yes | Weather=Overcast, Temp=Mild) = \\frac{0.1111 \\times 0.64 }{0.07} = 1.0\n",
    "\\end{equation}\n",
    "\n",
    "And as before, we will not show again, but since we always play when the whether is overcast, the conditional probability ends up being 0 \n",
    "for this data for the question if we will not play given weather is overcast and temperature is mild.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Building in Scikit-Learn\n",
    "\n",
    "In this example, we will continue to use the dummy dataset with three columns: weather, temperature, and play.\n",
    "The first two are features(weather, temperature) and the other is the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning features and label variables\n",
    "weather=['Sunny','Sunny','Overcast','Rainy','Rainy','Rainy','Overcast','Sunny','Sunny',\n",
    "'Rainy','Sunny','Overcast','Overcast','Rainy']\n",
    "temp=['Hot','Hot','Hot','Mild','Cool','Cool','Cool','Mild','Cool','Mild','Mild','Mild','Hot','Mild']\n",
    "\n",
    "play=['No','No','Yes','Yes','Yes','No','Yes','No','Yes','Yes','Yes','Yes','Yes','No']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Features\n",
    "\n",
    "First, we need to convert these string labels into numbers. for example: 'Overcast', 'Rainy', 'Sunny' as 0, 1, 2.\n",
    "This is known as label encoding. Scikit-learn provides LabelEncoder library for encoding labels with a value between\n",
    "0 and one less than the number of discrete classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 0 1 1 1 0 2 2 1 2 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#creating labelEncoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "# Converting string labels into numbers.\n",
    "weather_encoded=le.fit_transform(weather)\n",
    "print(weather_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can encode the other columns as well using this simple encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temp: [1 1 1 2 0 0 0 2 0 2 2 2 1 2]\n",
      "Play: [0 0 1 1 1 0 1 0 1 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "temp_encoded=le.fit_transform(temp)\n",
    "label=le.fit_transform(play)\n",
    "print(\"Temp:\",temp_encoded)\n",
    "print(\"Play:\",label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we combine our features (weather and temp) into a single array (actually list of tuples currently):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 2)\n"
     ]
    }
   ],
   "source": [
    "#Combinig weather and temp into single listof tuples\n",
    "features = np.zeros( (len(label), 2) )\n",
    "features[:,0] = weather_encoded\n",
    "features[:,1] = temp_encoded\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Model\n",
    "\n",
    "Now we will generate a model using a naive Bayes classifer using the sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Value: [1]\n"
     ]
    }
   ],
   "source": [
    "#Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "model = GaussianNB()\n",
    "\n",
    "# Train the model using the training sets\n",
    "model.fit(features,label)\n",
    "\n",
    "#Predict Output\n",
    "predicted= model.predict([[0,2]]) # 0:Overcast, 2:Mild\n",
    "print(\"Predicted Value:\", predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00770751, 0.99229249]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba([[0,2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes with Multiple Labels\n",
    "\n",
    "Naive bayes can be used for multinomial (multiple label) classification tasks.  Till now you have learned Naive Bayes classification with binary\n",
    "labels. Now you will learn about multiple class classification in Naive Bayes. For example, if you want to classify a news article about technology,\n",
    "entertainment, politics, or sports.\n",
    "\n",
    "In model building part, we will use wine dataset, which is a very famous multi-class classification problem.\n",
    "This dataset is the result of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars.\n",
    "\n",
    "Lets first load the required wine dataset from scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import scikit-learn dataset library\n",
    "from sklearn import datasets\n",
    "\n",
    "#Load dataset\n",
    "wine = datasets.load_wine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Data\n",
    "\n",
    "We can print the target and feature names, to make sure you have the right dataset, as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n",
      "Labels:  ['class_0' 'class_1' 'class_2']\n"
     ]
    }
   ],
   "source": [
    "# print the names of the 13 features\n",
    "print(\"Features: \", wine.feature_names)\n",
    "\n",
    "# print the label type of wine(class_0, class_1, class_2)\n",
    "print(\"Labels: \", wine.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a good idea to always explore your data a bit, so you know what you're working with. Here, you can see the first\n",
    "five rows of the dataset are printed, as well as the target variable for the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 13)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print data(feature)shape\n",
    "wine.data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  14.23,    1.71,    2.43,   15.6 ,  127.  ,    2.8 ,    3.06,\n",
       "           0.28,    2.29,    5.64,    1.04,    3.92, 1065.  ],\n",
       "       [  13.2 ,    1.78,    2.14,   11.2 ,  100.  ,    2.65,    2.76,\n",
       "           0.26,    1.28,    4.38,    1.05,    3.4 , 1050.  ],\n",
       "       [  13.16,    2.36,    2.67,   18.6 ,  101.  ,    2.8 ,    3.24,\n",
       "           0.3 ,    2.81,    5.68,    1.03,    3.17, 1185.  ],\n",
       "       [  14.37,    1.95,    2.5 ,   16.8 ,  113.  ,    3.85,    3.49,\n",
       "           0.24,    2.18,    7.8 ,    0.86,    3.45, 1480.  ],\n",
       "       [  13.24,    2.59,    2.87,   21.  ,  118.  ,    2.8 ,    2.69,\n",
       "           0.39,    1.82,    4.32,    1.04,    2.93,  735.  ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the wine data features (top 5 records)\n",
    "wine.data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Data\n",
    "\n",
    "Lets split the data into train and test data sets as usual, so we can evaluate our classifiers\n",
    "performance more accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.3,random_state=109) # 70% training and 30% test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Generation\n",
    "\n",
    "After splitting, we create a naive Bayes model on the training set and perform prediction on test set features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "#Train the model using the training sets\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model\n",
    "\n",
    "After model generation let's check the accuracy using actual and predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9074074074074074\n"
     ]
    }
   ],
   "source": [
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3-datasci",
   "language": "python",
   "name": "python3-datasci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
